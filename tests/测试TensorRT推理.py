#!/usr/bin/env python3
"""
å¿«é€Ÿæµ‹è¯•TensorRTå¼•æ“æ˜¯å¦æ­£å¸¸å·¥ä½œ
"""

import time
import numpy as np

try:
    import tensorrt as trt
    import pycuda.driver as cuda
    import pycuda.autoinit
    print("âœ“ TensorRTå’ŒPyCUDAå¯ç”¨")
except ImportError as e:
    print(f"âœ— å¯¼å…¥å¤±è´¥: {e}")
    exit(1)

# åŠ è½½å¼•æ“
engine_path = "models/yolov11_host.engine"
print(f"\nåŠ è½½å¼•æ“: {engine_path}")

logger = trt.Logger(trt.Logger.WARNING)

with open(engine_path, 'rb') as f:
    engine_data = f.read()

runtime = trt.Runtime(logger)
engine = runtime.deserialize_cuda_engine(engine_data)

if engine is None:
    print("âœ— å¼•æ“åŠ è½½å¤±è´¥")
    exit(1)

print("âœ“ å¼•æ“åŠ è½½æˆåŠŸ")

# è·å–è¾“å…¥è¾“å‡ºä¿¡æ¯
input_name = engine.get_tensor_name(0)
output_name = engine.get_tensor_name(1)
input_shape = engine.get_tensor_shape(input_name)
output_shape = engine.get_tensor_shape(output_name)

print(f"  è¾“å…¥: {input_name} {list(input_shape)}")
print(f"  è¾“å‡º: {output_name} {list(output_shape)}")

# åˆ›å»ºæ‰§è¡Œä¸Šä¸‹æ–‡
context = engine.create_execution_context()
print("âœ“ æ‰§è¡Œä¸Šä¸‹æ–‡åˆ›å»ºæˆåŠŸ")

# åˆ†é…GPUå†…å­˜
input_size = trt.volume(input_shape) * 4  # float32
output_size = trt.volume(output_shape) * 4

d_input = cuda.mem_alloc(input_size)
d_output = cuda.mem_alloc(output_size)
stream = cuda.Stream()

print(f"âœ“ GPUå†…å­˜åˆ†é…å®Œæˆ")
print(f"  è¾“å…¥: {input_size / 1024 / 1024:.1f} MB")
print(f"  è¾“å‡º: {output_size / 1024 / 1024:.1f} MB")

# å‡†å¤‡æµ‹è¯•æ•°æ®
print("\nå‡†å¤‡æµ‹è¯•æ•°æ®...")
input_data = np.random.randn(*input_shape).astype(np.float32)
print(f"âœ“ æµ‹è¯•æ•°æ®: {input_data.shape}")

# æ‰§è¡Œæ¨ç†
print("\næ‰§è¡Œæ¨ç†...")
cuda.memcpy_htod_async(d_input, input_data, stream)

context.set_tensor_address(input_name, int(d_input))
context.set_tensor_address(output_name, int(d_output))

start = time.time()
context.execute_async_v3(stream_handle=stream.handle)
stream.synchronize()
elapsed = time.time() - start

output_data = np.empty(output_shape, dtype=np.float32)
cuda.memcpy_dtoh_async(output_data, d_output, stream)
stream.synchronize()

print(f"âœ“ æ¨ç†æˆåŠŸï¼")
print(f"  è€—æ—¶: {elapsed*1000:.2f} ms")
print(f"  è¾“å‡ºå½¢çŠ¶: {output_data.shape}")
print(f"  è¾“å‡ºèŒƒå›´: [{output_data.min():.3f}, {output_data.max():.3f}]")

# å¤šæ¬¡æµ‹è¯•æ€§èƒ½
print("\næ€§èƒ½æµ‹è¯•ï¼ˆ10æ¬¡ï¼‰...")
times = []
for i in range(10):
    cuda.memcpy_htod_async(d_input, input_data, stream)
    context.set_tensor_address(input_name, int(d_input))
    context.set_tensor_address(output_name, int(d_output))
    
    start = time.time()
    context.execute_async_v3(stream_handle=stream.handle)
    stream.synchronize()
    elapsed = time.time() - start
    times.append(elapsed)

avg_time = np.mean(times) * 1000
fps = 1.0 / np.mean(times)

print(f"âœ“ å¹³å‡æ¨ç†æ—¶é—´: {avg_time:.2f} ms")
print(f"âœ“ é¢„æœŸFPS: {fps:.1f}")

print("\n" + "="*60)
print("ğŸ‰ TensorRTå¼•æ“å·¥ä½œæ­£å¸¸ï¼")
print("="*60)


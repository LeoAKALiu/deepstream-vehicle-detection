#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¤„ç†è§†é¢‘æ–‡ä»¶å¹¶ç”Ÿæˆæ£€æµ‹ç»“æœå›¾
ç”¨äºä»æŸåæˆ–ä¸å®Œæ•´çš„è§†é¢‘ä¸­æå–å¸§å¹¶è¿è¡Œæ£€æµ‹ç®—æ³•
"""

import sys
import os
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'python_apps'))

import cv2
import numpy as np
import argparse
from datetime import datetime
import json

# å¯¼å…¥æ£€æµ‹ç›¸å…³æ¨¡å—
from tensorrt_yolo_inference import TensorRTInference
from byte_tracker import ByteTracker
from config_loader import get_config

try:
    import hyperlpr3 as lpr3
    LPR_AVAILABLE = True
except ImportError:
    LPR_AVAILABLE = False
    print("âš  HyperLPR3æœªå®‰è£…ï¼Œè½¦ç‰Œè¯†åˆ«åŠŸèƒ½å°†ä¸å¯ç”¨")

# è‡ªå®šä¹‰æ¨¡å‹ç±»åˆ«
CUSTOM_CLASSES = {
    0: 'excavator',       # æŒ–æ˜æœº
    1: 'bulldozer',       # æ¨åœŸæœº
    2: 'roller',          # å‹è·¯æœº
    3: 'loader',          # è£…è½½æœº
    4: 'dump-truck',      # è‡ªå¸è½¦
    5: 'concrete-mixer',  # æ··å‡åœŸæ…æ‹Œè½¦
    6: 'pump-truck',      # æ³µè½¦
    7: 'truck',           # å¡è½¦
    8: 'crane',           # èµ·é‡æœº
    9: 'car',             # å°æ±½è½¦
}

VEHICLE_CLASSES = {
    'excavator': 'construction',
    'bulldozer': 'construction',
    'roller': 'construction',
    'loader': 'construction',
    'dump-truck': 'construction',
    'concrete-mixer': 'construction',
    'pump-truck': 'construction',
    'truck': 'construction',
    'crane': 'construction',
    'car': 'civilian',
}

COLORS = {
    'construction': (0, 140, 255),   # æ©™è‰²
    'civilian': (0, 255, 0),          # ç»¿è‰²
    'unregistered': (0, 0, 255),      # çº¢è‰²
}


def extract_frames_from_video(video_path: str, output_dir: str, max_frames: int = 50, interval: int = 30) -> list:
    """
    ä»è§†é¢‘ä¸­æå–å¸§ï¼ˆå³ä½¿è§†é¢‘æŸåä¹Ÿå°è¯•ï¼‰
    
    Args:
        video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•
        max_frames: æœ€å¤§æå–å¸§æ•°
        interval: å¸§é—´éš”
    
    Returns:
        æå–çš„å¸§åˆ—è¡¨
    """
    os.makedirs(output_dir, exist_ok=True)
    
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        print(f"âš  æ— æ³•æ‰“å¼€è§†é¢‘ï¼Œå°è¯•å¼ºåˆ¶è¯»å–...")
        # å°è¯•ä½¿ç”¨ä¸åŒçš„åç«¯
        cap = cv2.VideoCapture(video_path, cv2.CAP_FFMPEG)
    
    if not cap.isOpened():
        print(f"âŒ å®Œå…¨æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶: {video_path}")
        return []
    
    frames = []
    frame_count = 0
    saved_count = 0
    
    print(f"å¼€å§‹æå–å¸§...")
    
    while saved_count < max_frames:
        ret, frame = cap.read()
        if not ret:
            break
        
        frame_count += 1
        
        # æŒ‰é—´éš”ä¿å­˜å¸§
        if frame_count % interval == 0:
            frame_path = os.path.join(output_dir, f"frame_{frame_count:06d}.jpg")
            cv2.imwrite(frame_path, frame)
            frames.append((frame_count, frame_path, frame))
            saved_count += 1
            print(f"  ä¿å­˜å¸§ {frame_count}: {frame_path}")
    
    cap.release()
    print(f"âœ… å…±æå– {saved_count} å¸§")
    return frames


def process_frame_with_detection(frame: np.ndarray, inference: TensorRTInference, 
                                 tracker: ByteTracker, frame_id: int, config: dict) -> tuple:
    """
    å¯¹å•å¸§è¿›è¡Œæ£€æµ‹å’Œè·Ÿè¸ª
    
    Returns:
        (result_frame, detections_info)
    """
    # é¢„å¤„ç†
    input_h, input_w = config['detection']['input_resolution']
    frame_resized = cv2.resize(frame, (input_w, input_h))
    input_data = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2RGB)
    input_data = input_data.astype(np.float32) / 255.0
    input_data = np.transpose(input_data, (2, 0, 1))
    input_data = np.expand_dims(input_data, axis=0)
    
    # æ¨ç†
    output = inference.infer(input_data)
    boxes, confidences, class_ids = inference.postprocess(output)
    
    # è½¬æ¢åˆ°åŸå›¾åæ ‡
    h, w = frame.shape[:2]
    scale_x = w / input_w
    scale_y = h / input_h
    
    boxes_scaled = []
    for box in boxes:
        x1, y1, x2, y2 = box
        boxes_scaled.append([
            int(x1 * scale_x),
            int(y1 * scale_y),
            int(x2 * scale_x),
            int(y2 * scale_y)
        ])
    
    # è·Ÿè¸ª
    detections = []
    for i, (box, conf, cls_id) in enumerate(zip(boxes_scaled, confidences, class_ids)):
        x1, y1, x2, y2 = box
        detections.append([x1, y1, x2, y2, conf, cls_id])
    
    tracks = tracker.update(np.array(detections), frame)
    
    # ç»˜åˆ¶ç»“æœ
    result_frame = frame.copy()
    detections_info = []
    
    for track in tracks:
        track_id = int(track[4])
        x1, y1, x2, y2 = map(int, track[:4])
        cls_id = int(track[5])
        
        class_name = CUSTOM_CLASSES.get(cls_id, 'unknown')
        vehicle_type = VEHICLE_CLASSES.get(class_name, 'unknown')
        
        # é€‰æ‹©é¢œè‰²
        if vehicle_type == 'construction':
            color = COLORS['construction']
        elif vehicle_type == 'civilian':
            color = COLORS['civilian']
        else:
            color = (128, 128, 128)
        
        # ç»˜åˆ¶è¾¹ç•Œæ¡†
        cv2.rectangle(result_frame, (x1, y1), (x2, y2), color, 2)
        
        # æ ‡ç­¾
        label = f"{class_name} ID:{track_id}"
        label_size, _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)
        cv2.rectangle(result_frame, (x1, y1 - label_size[1] - 10), 
                     (x1 + label_size[0], y1), color, -1)
        cv2.putText(result_frame, label, (x1, y1 - 5),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        
        detections_info.append({
            'track_id': track_id,
            'class': class_name,
            'vehicle_type': vehicle_type,
            'bbox': [x1, y1, x2, y2],
            'confidence': float(track[4])
        })
    
    return result_frame, detections_info


def main():
    parser = argparse.ArgumentParser(description='å¤„ç†è§†é¢‘å¹¶ç”Ÿæˆæ£€æµ‹ç»“æœå›¾')
    parser.add_argument('--video', type=str, required=True, help='è§†é¢‘æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output', type=str, default='./results', help='è¾“å‡ºç›®å½•')
    parser.add_argument('--max-frames', type=int, default=20, help='æœ€å¤§å¤„ç†å¸§æ•°')
    parser.add_argument('--interval', type=int, default=30, help='å¸§é—´éš”')
    parser.add_argument('--model', type=str, default='models/custom_yolo.engine', help='æ¨¡å‹æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--labels', type=str, default='config/labels.txt', help='æ ‡ç­¾æ–‡ä»¶è·¯å¾„')
    
    args = parser.parse_args()
    
    # åŠ è½½é…ç½®
    config = get_config()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir = os.path.join(args.output, f"video_results_{timestamp}")
    os.makedirs(output_dir, exist_ok=True)
    
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    
    # åˆå§‹åŒ–æ£€æµ‹å™¨
    print("ğŸ”§ åˆå§‹åŒ–æ£€æµ‹å™¨...")
    # è·å–è¾“å…¥åˆ†è¾¨ç‡
    input_resolution = config.get('detection', {}).get('input_resolution', [640, 640])
    input_shape = (input_resolution[0], input_resolution[1])
    inference = TensorRTInference(args.model, input_shape=input_shape)
    
    # åˆå§‹åŒ–è·Ÿè¸ªå™¨
    tracker_config = config.get('tracking', {})
    tracker = ByteTracker(
        track_thresh=tracker_config.get('track_thresh', 0.6),
        high_thresh=tracker_config.get('high_thresh', 0.7),
        match_thresh=tracker_config.get('match_thresh', 0.7),
        track_buffer=tracker_config.get('track_buffer', 50),
        frame_rate=30
    )
    
    # æå–å¸§
    print(f"\nğŸ“¹ å¤„ç†è§†é¢‘: {args.video}")
    frames_dir = os.path.join(output_dir, "extracted_frames")
    frames = extract_frames_from_video(args.video, frames_dir, args.max_frames, args.interval)
    
    if not frames:
        print("âŒ æ— æ³•ä»è§†é¢‘ä¸­æå–å¸§")
        return
    
    # å¤„ç†æ¯ä¸€å¸§
    print(f"\nğŸ” è¿è¡Œæ£€æµ‹ç®—æ³•...")
    results_dir = os.path.join(output_dir, "detection_results")
    os.makedirs(results_dir, exist_ok=True)
    
    all_detections = []
    
    for idx, (frame_num, frame_path, frame) in enumerate(frames):
        print(f"  å¤„ç†å¸§ {idx+1}/{len(frames)} (åŸå¸§å·: {frame_num})...")
        
        result_frame, detections_info = process_frame_with_detection(
            frame, inference, tracker, frame_num, config
        )
        
        # ä¿å­˜ç»“æœå›¾
        result_path = os.path.join(results_dir, f"result_frame_{frame_num:06d}.jpg")
        cv2.imwrite(result_path, result_frame)
        
        all_detections.append({
            'frame': frame_num,
            'detections': detections_info
        })
        
        print(f"    âœ… ä¿å­˜ç»“æœ: {result_path} (æ£€æµ‹åˆ° {len(detections_info)} ä¸ªç›®æ ‡)")
    
    # ä¿å­˜æ£€æµ‹ç»“æœJSON
    results_json = os.path.join(output_dir, "detections.json")
    with open(results_json, 'w', encoding='utf-8') as f:
        json.dump(all_detections, f, indent=2, ensure_ascii=False)
    
    print(f"\nâœ… å¤„ç†å®Œæˆ!")
    print(f"  ç»“æœå›¾ç›®å½•: {results_dir}")
    print(f"  æ£€æµ‹ç»“æœJSON: {results_json}")
    print(f"  å…±å¤„ç† {len(frames)} å¸§")
    
    # ç»Ÿè®¡ä¿¡æ¯
    total_detections = sum(len(d['detections']) for d in all_detections)
    construction_count = sum(1 for d in all_detections 
                            for det in d['detections'] 
                            if det['vehicle_type'] == 'construction')
    civilian_count = sum(1 for d in all_detections 
                        for det in d['detections'] 
                        if det['vehicle_type'] == 'civilian')
    
    print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    print(f"  æ€»æ£€æµ‹æ•°: {total_detections}")
    print(f"  å·¥ç¨‹è½¦è¾†: {construction_count}")
    print(f"  ç¤¾ä¼šè½¦è¾†: {civilian_count}")


if __name__ == '__main__':
    main()

